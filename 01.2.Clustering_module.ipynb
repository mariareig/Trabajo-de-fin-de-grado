{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7c58c20-5c06-46e0-b1ff-f111ce56ec54",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 16:06:58.921672: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2024-05-08 16:06:58.925730: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2024-05-08 16:06:58.985274: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2024-05-08 16:07:00.063877: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/mnt/pqsashrd/data/ann_pred_1.csv/</td><td>ann_pred_1.csv/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/pqsashrd/data/ann_pred_1_test.csv/</td><td>ann_pred_1_test.csv/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/pqsashrd/data/ann_pred_3.csv/</td><td>ann_pred_3.csv/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/pqsashrd/data/clust_pred_1.csv/</td><td>clust_pred_1.csv/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/pqsashrd/data/clust_pred_1_test.csv/</td><td>clust_pred_1_test.csv/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/pqsashrd/data/clust_pred_3.csv/</td><td>clust_pred_3.csv/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/pqsashrd/data/clust_pred_3_test.csv/</td><td>clust_pred_3_test.csv/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/pqsashrd/data/test_1.csv</td><td>test_1.csv</td><td>38574669</td><td>1714569841000</td></tr><tr><td>dbfs:/mnt/pqsashrd/data/test_2.csv</td><td>test_2.csv</td><td>35056701</td><td>1714569835000</td></tr><tr><td>dbfs:/mnt/pqsashrd/data/test_3.csv</td><td>test_3.csv</td><td>25575183</td><td>1714569796000</td></tr><tr><td>dbfs:/mnt/pqsashrd/data/train_1.csv</td><td>train_1.csv</td><td>81417863</td><td>1714569888000</td></tr><tr><td>dbfs:/mnt/pqsashrd/data/train_1_partitioned/</td><td>train_1_partitioned/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/pqsashrd/data/train_2.csv</td><td>train_2.csv</td><td>73764410</td><td>1714569886000</td></tr><tr><td>dbfs:/mnt/pqsashrd/data/train_3.csv</td><td>train_3.csv</td><td>54130409</td><td>1714569868000</td></tr><tr><td>dbfs:/mnt/pqsashrd/data/train_3_partitioned/</td><td>train_3_partitioned/</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/mnt/pqsashrd/data/ann_pred_1.csv/",
         "ann_pred_1.csv/",
         0,
         0
        ],
        [
         "dbfs:/mnt/pqsashrd/data/ann_pred_1_test.csv/",
         "ann_pred_1_test.csv/",
         0,
         0
        ],
        [
         "dbfs:/mnt/pqsashrd/data/ann_pred_3.csv/",
         "ann_pred_3.csv/",
         0,
         0
        ],
        [
         "dbfs:/mnt/pqsashrd/data/clust_pred_1.csv/",
         "clust_pred_1.csv/",
         0,
         0
        ],
        [
         "dbfs:/mnt/pqsashrd/data/clust_pred_1_test.csv/",
         "clust_pred_1_test.csv/",
         0,
         0
        ],
        [
         "dbfs:/mnt/pqsashrd/data/clust_pred_3.csv/",
         "clust_pred_3.csv/",
         0,
         0
        ],
        [
         "dbfs:/mnt/pqsashrd/data/clust_pred_3_test.csv/",
         "clust_pred_3_test.csv/",
         0,
         0
        ],
        [
         "dbfs:/mnt/pqsashrd/data/test_1.csv",
         "test_1.csv",
         38574669,
         1714569841000
        ],
        [
         "dbfs:/mnt/pqsashrd/data/test_2.csv",
         "test_2.csv",
         35056701,
         1714569835000
        ],
        [
         "dbfs:/mnt/pqsashrd/data/test_3.csv",
         "test_3.csv",
         25575183,
         1714569796000
        ],
        [
         "dbfs:/mnt/pqsashrd/data/train_1.csv",
         "train_1.csv",
         81417863,
         1714569888000
        ],
        [
         "dbfs:/mnt/pqsashrd/data/train_1_partitioned/",
         "train_1_partitioned/",
         0,
         0
        ],
        [
         "dbfs:/mnt/pqsashrd/data/train_2.csv",
         "train_2.csv",
         73764410,
         1714569886000
        ],
        [
         "dbfs:/mnt/pqsashrd/data/train_3.csv",
         "train_3.csv",
         54130409,
         1714569868000
        ],
        [
         "dbfs:/mnt/pqsashrd/data/train_3_partitioned/",
         "train_3_partitioned/",
         0,
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run ./conf_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47a0ad91-b60f-46a5-b42b-78bb7b95a559",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03e020e6-35a3-40a9-bb55-325bf5a7b4a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_train = ps.read_csv(train_3_csv_file_path)\n",
    "df_train = df_train.drop('_c0', axis=1)\n",
    "\n",
    "df_test = ps.read_csv(test_3_csv_file_path)\n",
    "df_test = df_test.drop('_c0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "047798e0-b110-41d4-b7d0-60481ed168e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "df_train = df_train.to_pandas()\n",
    "df_test = df_test.to_pandas()\n",
    "\n",
    "df_train[\"attack_cat_encoded\"] = label_encoder.fit_transform(df_train[\"attack_cat\"])\n",
    "df_test[\"attack_cat_encoded\"] = label_encoder.transform(df_test[\"attack_cat\"])\n",
    "\n",
    "df_train = ps.DataFrame(df_train)\n",
    "df_test = ps.DataFrame(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28d2efc8-d8b0-4541-8b58-d439180a72ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_train = df_train.set_index(\"attack_cat\")\n",
    "X_train = X_train.drop(\"attack_cat_encoded\", axis=1)\n",
    "y_train = df_train[\"attack_cat_encoded\"]\n",
    "\n",
    "X_test = df_test.set_index(\"attack_cat\")\n",
    "X_test = X_test.drop(\"attack_cat_encoded\", axis=1)\n",
    "y_test = df_test[\"attack_cat_encoded\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60e8295e-0dfb-4c86-818a-aecfd6b2b4b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "Adjusted Rand Index (ARI): This metric measures the similarity between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings. The ARI is adjusted for chance, which makes it more robust for comparing different clustering algorithms.\n",
    "\n",
    "Silhouette Coefficient: This metric measures how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The Silhouette Coefficient is calculated for each sample and can range from -1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. If most objects have a high value, the clustering configuration is appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5b58d8a-4de8-4b1b-bfc9-dd6f6de9aa10",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Fuzzy K-MEANS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "238ecea3-c990-4881-8404-f6c12183e844",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "estimator = LDA(n_components = 2)\n",
    "X_lda = estimator.fit_transform(ps.DataFrame(X_train).to_pandas(), y_train.to_pandas())\n",
    "X_lda_df = ps.DataFrame(data=X_lda, columns=['LDA_component_1', 'LDA_component_2'], index=ps.DataFrame(X_train).index)\n",
    "X_lda_df = X_lda_df.to_pandas()\n",
    "\n",
    "X_lda_test = estimator.transform(ps.DataFrame(X_test).to_pandas())\n",
    "X_lda_df_test_ = ps.DataFrame(data=X_lda_test, columns=['LDA_component_1', 'LDA_component_2'], index=ps.DataFrame(X_test).index)\n",
    "X_lda_df_test_ = X_lda_df_test_.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f31711a4-a5b1-4376-b17d-bb0902f78b3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "custom_color_scale = [\n",
    "    '#800080',  # Purple (no change)\n",
    "    '#9370db',  # Medium Purple (no change)\n",
    "    '#6495ed',  # Cornflower Blue (no change)\n",
    "    '#000080',  # Navy (no change)\n",
    "    '#6998b3',  # Darker Light Blue (no change)\n",
    "    \"#e4cbd1\",  # Medium Violet Red (a darker, more subdued pink)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b955a6bf-b5bb-4e08-81b7-4bc28fa2a8c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "estimator = LDA(n_components = 4)\n",
    "X_lda = estimator.fit_transform(X_train.to_pandas(), y_train.to_pandas())\n",
    "X_lda_df_ = ps.DataFrame(data=X_lda, columns=['LDA_component_1', 'LDA_component_2', 'LDA_component_3', 'LDA_component_4'], index=X_train.index)\n",
    "X_lda_df_ = X_lda_df_.to_pandas()\n",
    "\n",
    "X_lda_test = estimator.transform(X_test.to_pandas())\n",
    "X_lda_test_df = ps.DataFrame(data=X_lda_test, columns=['LDA_component_1', 'LDA_component_2', 'LDA_component_3', 'LDA_component_4'], index=X_test.index)\n",
    "X_lda_test_df = X_lda_test_df.to_pandas()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abddb930-43fd-474a-96ea-2efc233b6faa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters = 5: Silhouette Coefficient = 0.8649812088089857, Adjusted Rand Index (ARI) = 0.5502212008666248\n"
     ]
    }
   ],
   "source": [
    "k=5\n",
    "cntr, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(X_lda_df_.T, c=k, m=2, error=0.00001, maxiter=100000)\n",
    "predicted_labels = np.argmax(u, axis=0)\n",
    "\n",
    "true_labels = y_train.to_numpy()\n",
    "ari = metrics.adjusted_rand_score(true_labels, predicted_labels)\n",
    "\n",
    "spark_df = spark.createDataFrame([(Vectors.dense(x), int(l)) for x, l in zip(X_lda_df_.values, predicted_labels)], [\"features\", \"prediction\"])\n",
    "evaluator = ClusteringEvaluator(predictionCol='prediction', featuresCol='features', metricName='silhouette', distanceMeasure='squaredEuclidean')\n",
    "silhouette_score = evaluator.evaluate(spark_df)\n",
    "\n",
    "print(f\"Number of clusters = {k}: Silhouette Coefficient = {silhouette_score}, Adjusted Rand Index (ARI) = {ari}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb7fe0a7-fc6f-4bf5-af4f-bffbc10c3b9d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters = 5: Silhouette Coefficient = 0.8688813108656244, Adjusted Rand Index (ARI) = 0.43052079646434704\n"
     ]
    }
   ],
   "source": [
    "u_test, u0_test, d_test, jm_test, p_test, fpc_test = cmeans_predict(X_lda_test_df.T, cntr, m=5, error=0.00001, maxiter=100000)\n",
    "predicted_labels_test = np.argmax(u_test, axis=0)\n",
    "\n",
    "true_labels = y_test.to_numpy()\n",
    "ari = metrics.adjusted_rand_score(true_labels, predicted_labels_test)\n",
    "\n",
    "spark_df = spark.createDataFrame([(Vectors.dense(x), int(l)) for x, l in zip(X_lda_test_df.values, predicted_labels_test)], [\"features\", \"prediction\"])\n",
    "evaluator = ClusteringEvaluator(predictionCol='prediction', featuresCol='features', metricName='silhouette', distanceMeasure='squaredEuclidean')\n",
    "silhouette_score = evaluator.evaluate(spark_df)\n",
    "\n",
    "print(f\"Number of clusters = {k}: Silhouette Coefficient = {silhouette_score}, Adjusted Rand Index (ARI) = {ari}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "753f4764-6316-48b6-b7f5-b949eed7e040",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def map_predicted_labels(num):\n",
    "    if num == 0:\n",
    "        return 2\n",
    "    elif num == 1:\n",
    "        return 4\n",
    "    elif num == 2:\n",
    "        return 0\n",
    "    elif num == 3:\n",
    "        return 1\n",
    "    elif num == 4:\n",
    "        return 3\n",
    "    else:\n",
    "        return num\n",
    "\n",
    "predicted_labels_transformed = list(map(map_predicted_labels, predicted_labels))\n",
    "predicted_labels_transformed_test = list(map(map_predicted_labels, predicted_labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07ef7591-a369-4638-a885-1fa50b66580e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_lda_df_ = X_lda_df_.reset_index()\n",
    "X_lda_df_[\"clustering_pred\"] = predicted_labels_transformed\n",
    "soft_classification = pd.DataFrame(u.T, columns=['clustering_prob_0', 'clustering_prob_1', 'clustering_prob_2', 'clustering_prob_3', 'clustering_prob_4'])\n",
    "X_lda_df_ = pd.concat([X_lda_df_, soft_classification], axis=1)\n",
    "\n",
    "X_lda_test_df = X_lda_test_df.reset_index()\n",
    "X_lda_test_df[\"clustering_pred\"] = predicted_labels_transformed_test\n",
    "soft_classification_test = pd.DataFrame(u_test.T, columns=['clustering_prob_0', 'clustering_prob_1', 'clustering_prob_2', 'clustering_prob_3', 'clustering_prob_4'])\n",
    "X_lda_test_df = pd.concat([X_lda_test_df, soft_classification_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "605058f1-36a2-447d-ac9f-6ec380025c1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_lda_df_ = ps.DataFrame(X_lda_df_).to_spark()\n",
    "save_path = clust_pred_3_csv_file_path\n",
    "X_lda_df_.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f51f449-e4c7-4ecc-a31c-9eaa88e0f4f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_lda_test_df = ps.DataFrame(X_lda_test_df).to_spark()\n",
    "save_path = clust_pred_3_test_csv_file_path\n",
    "X_lda_test_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18a20720-85b7-44d9-9bc6-8e1f5f06a8bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4108134970157439,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01.2.Clustering_module",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
